[toc]

# Webserver

![异常模型](.\img\model.png)

##定时器

webserver中定时器的一个作用是处理非活动链接。服务器程序需要定期处理非活动链接：给客户端发一个重连请求，或者关闭该连接，当然，在linux内核提供了对连接是否处于活动状态的定期检查机制，可以通过KEEPALIVE选项来激活，可这会使得应用程序对连接的管理变得复杂，因此，我们可以考虑在应用层实现这种机制。

利用alarm函数周期性地触发SIGALARM信号，信号地函数通知主循环执行定时器链表上的定时任务--关闭非活动的连接

简单的基于排序链表的定时器很垃圾，所以有两种方案：

### 时间轮

![image-20220514160416754](.\img\image-20220514160416754.png)

基于哈希，将定时器分配到N个槽

###时间堆

将定时器中超时时间最小的一个定时器的超时值作为心搏间隔，使用最小堆

![image-20220514160602605](.\img\image-20220514160602605.png)

## 杂项

为什么设置socket为非阻塞？

1.对于读写socket

当 connfd 或 clientfd 设置成阻塞模式时：send 函数会尝试发送数据，如果对端因为 TCP 窗口太小导致本端无法将数据发送出去，send 函数会一直阻塞直到对端 TCP 窗口变大足以发数据或者超时；recv 函数则正好相反，如果此时没有数据可收获，recv函数会一直阻塞直到收取到数据或者超时，有的话，取到数据后返回。send 和 recv 函数的超时时间可以分别使用 SO_SNDTIMEO 和 SO_RCVTIMEO 两个 socket 选项来设置。示例代码如下：

当 connfd 或 clientfd 设置成阻塞模式时：send 函数会尝试发送数据，如果对端因为 TCP 窗口太小导致本端无法将数据发送出去，send 函数会一直阻塞直到对端 TCP 窗口变大足以发数据或者超时；recv 函数则正好相反，如果此时没有数据可收获，recv函数会一直阻塞直到收取到数据或者超时，有的话，取到数据后返回。send 和 recv 函数的超时时间可以分别使用 SO_SNDTIMEO 和 SO_RCVTIMEO 两个 socket 选项来设置。示例代码如下：

![image-20220514171626582](.\img\image-20220514171626582.png)

2.对于监听socket

当 listenfd 设置成阻塞模式（默认行为，无需额外设置）时，如果连接 pending 队列中有需要处理的连接，accept 函数会立即返回，否则会一直阻塞下去，直到有新的连接到来

当 listenfd 设置成非阻塞模式，无论连接 pending 队列中是否有需要处理的连接，accept 都会立即返回，不会阻塞。如果有连接，则 accept 返回一个大于 0 的值，这个返回值即是我们上文所说的 clientfd；如果没有连接，accept 返回值小于 0，错误码 errno 为 EWOULDBLOCK

#BRPC

## 什么是rpc

互联网上的大多数机器通过TCP/IP相互通信。然而，TCP/IP 只保证可靠的数据传输。我们需要更多抽象来构建服务：

- 数据传输的格式是什么？不同的机器和网络可能有不同的字节顺序，直接发送内存数据是不合适的。数据中的字段是逐渐添加、修改或删除的，新服务如何与旧服务对话？
- TCP连接可以重复用于多个请求以减少开销吗？一个TCP连接可以同时发送多个请求吗？
- 如何与具有许多机器的集群通信？
- 连接断开时我该怎么办？如果服务器没有响应怎么办？
- ...

[RPC](https://en.wikipedia.org/wiki/Remote_procedure_call)通过将网络通信抽象为“客户端访问服务器上的功能”来解决上述问题：客户端向服务器发送请求，等待服务器接收 -> 处理 -> 响应请求，然后根据结果执行操作。

![rpc.png](\img\rpc.png)

让我们看看问题是如何解决的。

- RPC 需要由[protobuf](https://github.com/google/protobuf)很好地完成的序列化。用户以 protobuf::Message 格式填写请求，进行 RPC，并从 protobuf::Message 中的响应中获取结果。protobuf 具有良好的前向和后向兼容性，供用户更改字段和增量构建服务。对于 http 服务，[json](http://www.json.org/)被广泛用于序列化。
- 连接的建立和重用对用户是透明的，但用户可以选择[不同的连接类型](https://github.com/apache/incubator-brpc/blob/master/docs/en/client.md#connection-type)：短连接、池连接、单一连接。
- 机器由命名服务发现，可以通过[DNS](https://en.wikipedia.org/wiki/Domain_Name_System)、[ZooKeeper](https://zookeeper.apache.org/)或[etcd 实现](https://github.com/coreos/etcd)。在百度内部，我们使用 BNS（Baidu Naming Service）。brpc 也提供[“list://”和“file://”](https://github.com/apache/incubator-brpc/blob/master/docs/en/client.md#naming-service)。用户指定负载均衡算法，为所有机器的每个请求选择一台机器，包括：循环、随机、[一致散列](https://github.com/apache/incubator-brpc/blob/master/docs/cn/consistent_hashing.md)（murmurhash3 或 md5）和[位置感知](https://github.com/apache/incubator-brpc/blob/master/docs/cn/lalb.md)。
- RPC 在连接中断时重试。当服务器在给定时间内没有响应时，客户端会因超时错误而失败。

## 同步异步rpc

### 同步

### 异步

提升吞吐量，其实关键就两个字：“异步”，提高CPU等资源的利用率

异步，最常用的方式就是返回 Future 对象的 Future 方式，或者入参为 Callback 对象的回调方式，而 Future 方式可以说是最简单的一种异步方式了。我们发起一次异步请求并且从请求上下文中拿到一个 Future，之后我们就可以调用 Future 的 get 方法获取结果。

对于 RPC 框架，无论是同步调用还是异步调用，调用端的内部实现都是异步的

调用端发送的每条消息都一个唯一的消息标识，实际上调用端向服务端发送请求消息之前会先创建一个 Future，并会存储这个消息标识与这个 Future 的映射，动态代理所获得的返回值最终就是从这个 Future 中获取的；当收到服务端响应的消息时，调用端会根据响应消息的唯一标识，通过之前存储的映射找到对应的 Future，将结果注入给那个 Future，再进行一系列的处理逻辑，最后动态代理从 Future 中获得到正确的返回值。

- **所谓的同步调用**，不过是 RPC 框架在调用端的处理逻辑中主动执行了这个 Future 的 get 方法，让动态代理等待返回值；而异步调用则是 RPC 框架没有主动执行这个 Future 的 get 方法，用户可以从请求上下文中得到这个 Future，自己决定什么时候执行这个 Future 的 get 方法。

# QT项目

可以回答多线程同步

1. 运行在控制节点上的master，这个程序监视并控制整个机群的状态
2. 运行在每个计算节点的slave，负责监控资源的状态

master和每个slave之间用一个TCP连接，每个master采用2或4个IO线程来处理8个TCP connection，能有效降低延迟

master要异步地网本地硬盘写log，要求logging library有自己的IO线程

master有可能要独写数据库库，数据库的第三方library得有自己的线程

master要服务于多个clients，需要两个IO线程专门处理和clients的通信



消息队列的实现

4 无锁队列
对于消息队列可以采用有锁来实现，多线程操作有锁队列也会引起的问题：
1 同一个线程在不同cpu运行会发生切换 cache损坏(cache trashing)
2 在同步机制上争抢队列block_queue(mutex+condition)
3 动态分配内存(对于高性能队列，开辟释放内存，不是直接调用系统调用来实现，而是采用内存池来实现）


## 日志

在多线程程序中，前端和后端都与单线程程序无甚区别，无非是每个线程有自己的前端，整个程序共用一个后端。难点是要将日志数据从多个前端高效地传输到后端。这是一个典型的多生产者-单消费者问题



要给web写一个sigpip  handler  什么也不做

我的一个服务器程序, 在Windows下运行正常.

但当在Linux(centos 6.3)下,进行对端未开启的异常测试时,出现莫名退出 . 最后跟踪到是write调用导致退出. 用gdb执行程序, 退出时提示"Broken pipe".

 

问题分析：

对一个对端已经关闭的socket调用两次write, 第二次将会生成SIGPIPE信号, 该信号默认结束进程.

具体的分析可以结合TCP的”四次握手”关闭. TCP是全双工的信道, 可以看作两条单工信道, TCP连接两端的两个端点各负责一条. 当对端调用close时, 虽然本意是关闭整个两条信道, 但本端只是收到FIN包. 按照TCP

协议的语义, 表示对端只是关闭了其所负责的那一条单工信道, 仍然可以继续接收数据. 也就是说, **因为TCP协议的限制, 一个端点无法获知对端已经完全关闭**.

1.压测把redis写满了 设置redis最大内存和过期时间

2.netconf  ssh连接的LRU缓存方案

3.http短连接太多，端口用完了，

```
/etc/sysctrl.conf
# 扩大端口范围，增加端口资源
net.ipv4.ip_local_port_range = 1024  65535
#开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0
net.ipv4.tcp_tw_reuse = 1
#开启TCP连接中TIME-WAIT sockets的快速回收，默认为0
net.ipv4.tcp_tw_recycle = 1 #服务端主动发起关闭后等待的超时时间net.ipv4.tcp_fin_timeout = 5#开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击net.ipv4.tcp_syncookies = 1
```

当然，最好还是改成长连接，短时间大量短连接，而nodejs的垃圾回收又是周期性的，内存在高并发下直接拉满就不好了
